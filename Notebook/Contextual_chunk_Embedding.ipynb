{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0mr8UPQFHwaR"
   },
   "outputs": [],
   "source": [
    "# !pip install voyageai\n",
    "# !pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wXg8Nc2TItFR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i-A8Pp2YLU3T"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3IozkUn8ItIj"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Contextualized Embedding RAG Pipeline using Voyage Context-3\n",
    "Designed for comparing performance against traditional embeddings\n",
    "\"\"\"\n",
    "\n",
    "import voyageai\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "# from langchain.text_splitter import SentenceSplitter # Corrected import\n",
    "import json\n",
    "import time\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    \"\"\"Represents a document with metadata\"\"\"\n",
    "    id: str\n",
    "    title: str\n",
    "    content: str\n",
    "    metadata: Dict\n",
    "    doc_type: str  # 'legal', 'technical', 'financial', etc.\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    \"\"\"Represents a document chunk\"\"\"\n",
    "    text: str\n",
    "    doc_id: str\n",
    "    chunk_id: int\n",
    "    start_char: int\n",
    "    end_char: int\n",
    "    metadata: Dict\n",
    "\n",
    "@dataclass\n",
    "class RetrievalResult:\n",
    "    \"\"\"Results from retrieval with metrics\"\"\"\n",
    "    chunks: List[Tuple[Chunk, float]]  # (chunk, similarity_score)\n",
    "    latency_ms: float\n",
    "    method: str  # 'contextualized' or 'standard'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2bdqFAoUHTo"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class SentenceSplitter:\n",
    "    \"\"\"Custom sentence splitter - just copy this class\"\"\"\n",
    "    def split_text(self, text):\n",
    "        # Split on . ! ? followed by space\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "        return [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "# Use it in your pipeline:\n",
    "# self.text_splitter = SentenceSplitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fe-UR2tpI4CX"
   },
   "outputs": [],
   "source": [
    "class ContextualizedRAGPipeline:\n",
    "    \"\"\"\n",
    "    Production-ready RAG pipeline comparing contextualized vs standard embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str, chunk_size: int = 512, chunk_overlap: int = 0):\n",
    "        self.client = voyageai.Client(api_key=userdata.get('VOYAGE'))\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.text_splitter = SentenceSplitter()\n",
    "\n",
    "\n",
    "        # Storage for embeddings and chunks\n",
    "        self.chunks_store: List[Chunk] = []\n",
    "        self.contextualized_embeddings: List[np.ndarray] = []\n",
    "        self.standard_embeddings: List[np.ndarray] = []\n",
    "        self.documents: Dict[str, Document] = {}\n",
    "\n",
    "    def chunk_document(self, document: Document) -> List[Chunk]:\n",
    "        \"\"\"\n",
    "        Chunk a document while preserving structure and metadata\n",
    "        \"\"\"\n",
    "        # Split text into chunks\n",
    "        text_chunks = self.text_splitter.split_text(document.content)\n",
    "\n",
    "        chunks = []\n",
    "        current_pos = 0\n",
    "\n",
    "        for i, text in enumerate(text_chunks):\n",
    "            # Find actual position in original document\n",
    "            start_pos = document.content.find(text, current_pos)\n",
    "            end_pos = start_pos + len(text)\n",
    "\n",
    "            chunk = Chunk(\n",
    "                text=text,\n",
    "                doc_id=document.id,\n",
    "                chunk_id=i,\n",
    "                start_char=start_pos,\n",
    "                end_char=end_pos,\n",
    "                metadata={\n",
    "                    **document.metadata,\n",
    "                    'doc_title': document.title,\n",
    "                    'doc_type': document.doc_type,\n",
    "                    'chunk_position': i / len(text_chunks),  # Relative position\n",
    "                    'total_chunks': len(text_chunks)\n",
    "                }\n",
    "            )\n",
    "            chunks.append(chunk)\n",
    "            current_pos = end_pos\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def index_documents(self, documents: List[Document],\n",
    "                       use_contextualized: bool = True,\n",
    "                       use_standard: bool = True) -> Dict:\n",
    "        \"\"\"\n",
    "        Index documents using both contextualized and standard embeddings\n",
    "        \"\"\"\n",
    "        print(f\"Indexing {len(documents)} documents...\")\n",
    "\n",
    "        # Store documents\n",
    "        for doc in documents:\n",
    "            self.documents[doc.id] = doc\n",
    "\n",
    "        # Chunk all documents\n",
    "        all_chunks_by_doc = {}\n",
    "        for doc in documents:\n",
    "            chunks = self.chunk_document(doc)\n",
    "            all_chunks_by_doc[doc.id] = chunks\n",
    "            self.chunks_store.extend(chunks)\n",
    "\n",
    "        indexing_stats = {}\n",
    "\n",
    "        # Generate contextualized embeddings\n",
    "        if use_contextualized:\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Prepare input for contextualized embedding\n",
    "            # Group chunks by document\n",
    "            inputs = []\n",
    "            for doc_id, chunks in all_chunks_by_doc.items():\n",
    "                inputs.append([chunk.text for chunk in chunks])\n",
    "\n",
    "            # Get contextualized embeddings\n",
    "            embeds_obj = self.client.contextualized_embed(\n",
    "                inputs=inputs,\n",
    "                model=\"voyage-context-3\",\n",
    "                input_type=\"document\",\n",
    "                output_dimension=1024  # Using smaller dimension for efficiency\n",
    "            )\n",
    "\n",
    "            # Flatten embeddings while maintaining order\n",
    "            for result in embeds_obj.results:\n",
    "                self.contextualized_embeddings.extend(result.embeddings)\n",
    "\n",
    "            indexing_stats['contextualized'] = {\n",
    "                'time_seconds': time.time() - start_time,\n",
    "                'total_tokens': embeds_obj.total_tokens,\n",
    "                'embeddings_generated': len(self.contextualized_embeddings)\n",
    "            }\n",
    "\n",
    "        # Generate standard embeddings for comparison\n",
    "        if use_standard:\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Get all chunk texts\n",
    "            chunk_texts = [chunk.text for chunk in self.chunks_store]\n",
    "\n",
    "            # Standard embedding call\n",
    "            standard_embeds = self.client.embed(\n",
    "                texts=chunk_texts,\n",
    "                model=\"voyage-3-large\",\n",
    "                input_type=\"document\"\n",
    "            )\n",
    "\n",
    "            self.standard_embeddings = standard_embeds.embeddings\n",
    "\n",
    "            indexing_stats['standard'] = {\n",
    "                'time_seconds': time.time() - start_time,\n",
    "                'embeddings_generated': len(self.standard_embeddings)\n",
    "            }\n",
    "\n",
    "        return indexing_stats\n",
    "\n",
    "    def search(self, query: str, top_k: int = 10,\n",
    "              method: str = 'contextualized') -> RetrievalResult:\n",
    "        \"\"\"\n",
    "        Search for relevant chunks using specified embedding method\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        if method == 'contextualized':\n",
    "            # Embed query for contextualized search\n",
    "            query_embed = self.client.contextualized_embed(\n",
    "                inputs=[[query]],\n",
    "                model=\"voyage-context-3\",\n",
    "                input_type=\"query\",\n",
    "                output_dimension=1024\n",
    "            ).results[0].embeddings[0]\n",
    "\n",
    "            embeddings = self.contextualized_embeddings\n",
    "        else:\n",
    "            # Standard query embedding\n",
    "            query_embed = self.client.embed(\n",
    "                texts=[query],\n",
    "                model=\"voyage-3-large\",\n",
    "                input_type=\"query\"\n",
    "            ).embeddings[0]\n",
    "\n",
    "            embeddings = self.standard_embeddings\n",
    "\n",
    "        # Calculate similarities\n",
    "        similarities = np.dot(embeddings, query_embed)\n",
    "\n",
    "        # Get top-k indices\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "\n",
    "        # Prepare results\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            chunk = self.chunks_store[idx]\n",
    "            score = float(similarities[idx])\n",
    "            results.append((chunk, score))\n",
    "\n",
    "        latency_ms = (time.time() - start_time) * 1000\n",
    "\n",
    "        return RetrievalResult(\n",
    "            chunks=results,\n",
    "            latency_ms=latency_ms,\n",
    "            method=method\n",
    "        )\n",
    "\n",
    "    def compare_retrieval_methods(self, queries: List[str], top_k: int = 5) -> Dict:\n",
    "        \"\"\"\n",
    "        Compare contextualized vs standard retrieval across multiple queries\n",
    "        \"\"\"\n",
    "        comparison_results = []\n",
    "\n",
    "        for query in queries:\n",
    "            print(f\"\\nQuery: {query}\")\n",
    "\n",
    "            # Search with both methods\n",
    "            context_results = self.search(query, top_k, 'contextualized')\n",
    "            standard_results = self.search(query, top_k, 'standard')\n",
    "\n",
    "            # Compare top results\n",
    "            query_comparison = {\n",
    "                'query': query,\n",
    "                'contextualized': {\n",
    "                    'top_chunk': [c[0].text for c in context_results.chunks[:3]],# if context_results.chunks else None,\n",
    "                    'top_score': context_results.chunks[0][1] if context_results.chunks else 0,\n",
    "                    'latency_ms': context_results.latency_ms,\n",
    "                    'top_doc_ids': [c[0].doc_id for c in context_results.chunks[:3]]\n",
    "                },\n",
    "                'standard': {\n",
    "                    'top_chunk': [c[0].text for c in standard_results.chunks[:3]],\n",
    "                    'top_score': standard_results.chunks[0][1] if standard_results.chunks else 0,\n",
    "                    'latency_ms': standard_results.latency_ms,\n",
    "                    'top_doc_ids': [c[0].doc_id for c in standard_results.chunks[:3]]\n",
    "                }\n",
    "            }\n",
    "\n",
    "            comparison_results.append(query_comparison)\n",
    "\n",
    "            # Print summary\n",
    "            print(f\"  Contextualized top score: {query_comparison['contextualized']['top_score']:.4f}\")\n",
    "            print(f\"  Standard top score: {query_comparison['standard']['top_score']:.4f}\")\n",
    "            print(f\"  Score improvement: {(query_comparison['contextualized']['top_score'] - query_comparison['standard']['top_score']):.4f}\")\n",
    "\n",
    "        return {\n",
    "            'queries': comparison_results,\n",
    "            'summary': self._calculate_summary_metrics(comparison_results)\n",
    "        }\n",
    "\n",
    "    def _calculate_summary_metrics(self, results: List[Dict]) -> Dict:\n",
    "        \"\"\"Calculate summary metrics for comparison\"\"\"\n",
    "        contextualized_scores = [r['contextualized']['top_score'] for r in results]\n",
    "        standard_scores = [r['standard']['top_score'] for r in results]\n",
    "\n",
    "        return {\n",
    "            'avg_contextualized_score': np.mean(contextualized_scores),\n",
    "            'avg_standard_score': np.mean(standard_scores),\n",
    "            'avg_score_improvement': np.mean([c - s for c, s in zip(contextualized_scores, standard_scores)]),\n",
    "            'avg_latency_contextualized_ms': np.mean([r['contextualized']['latency_ms'] for r in results]),\n",
    "            'avg_latency_standard_ms': np.mean([r['standard']['latency_ms'] for r in results])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iBUcv4n5H5oW"
   },
   "outputs": [],
   "source": [
    "# Example usage for testing long technical documentation\n",
    "def create_test_documents() -> List[Document]:\n",
    "    \"\"\"\n",
    "    Create sample documents that showcase contextualized embedding benefits\n",
    "    \"\"\"\n",
    "    documents = [\n",
    "        Document(\n",
    "            id=\"aws_s3_docs\",\n",
    "            title=\"AWS S3 Technical Documentation\",\n",
    "            content=\"\"\"\n",
    "Amazon Simple Storage Service (Amazon S3) is an object storage service offering\n",
    "industry-leading scalability, data availability, security, and performance.\n",
    "\n",
    "Storage Classes:\n",
    "S3 Standard is designed for frequently accessed data with low latency requirements.\n",
    "It delivers low latency and high throughput performance.\n",
    "\n",
    "S3 Standard-IA (Infrequent Access) is designed for data that is accessed less\n",
    "frequently but requires rapid access when needed. It offers lower storage pricing\n",
    "than S3 Standard.\n",
    "\n",
    "S3 Glacier Instant Retrieval delivers low-cost storage for long-lived data that\n",
    "is rarely accessed and requires retrieval in milliseconds.\n",
    "\n",
    "Encryption:\n",
    "All Amazon S3 buckets have encryption configured by default. The default encryption\n",
    "uses server-side encryption with Amazon S3 managed keys (SSE-S3). You can also use\n",
    "server-side encryption with AWS KMS keys (SSE-KMS) or customer-provided keys (SSE-C).\n",
    "\n",
    "For SSE-KMS, AWS KMS generates and manages the cryptographic keys. KMS uses\n",
    "envelope encryption with AES-256-GCM. The data key is encrypted under a KMS key\n",
    "that never leaves AWS KMS unencrypted.\n",
    "\n",
    "Access Control:\n",
    "S3 Block Public Access provides settings for access points, buckets, and accounts\n",
    "to help you manage public access to Amazon S3 resources. By default, new buckets,\n",
    "access points, and objects don't allow public access.\n",
    "\n",
    "Bucket policies are JSON-based access policy language that can be used to add or\n",
    "deny permissions for different principals and actions. IAM policies specify what\n",
    "actions are allowed or denied on AWS resources.\n",
    "            \"\"\",\n",
    "            metadata={'source': 'AWS Documentation', 'version': '2024-Q4'},\n",
    "            doc_type='technical'\n",
    "        ),\n",
    "\n",
    "        Document(\n",
    "            id=\"azure_blob_docs\",\n",
    "            title=\"Azure Blob Storage Documentation\",\n",
    "            content=\"\"\"\n",
    "Azure Blob Storage is Microsoft's object storage solution for the cloud optimized\n",
    "for storing massive amounts of unstructured data.\n",
    "\n",
    "Storage Tiers:\n",
    "Hot tier is optimized for storing data that is accessed frequently. It offers the\n",
    "highest storage costs but lowest access costs.\n",
    "\n",
    "Cool tier is optimized for storing data that is infrequently accessed and stored\n",
    "for at least 30 days. It has lower storage costs but higher access costs than Hot.\n",
    "\n",
    "Archive tier is optimized for data that is rarely accessed and stored for at least\n",
    "180 days with flexible latency requirements. It offers the lowest storage costs.\n",
    "\n",
    "Encryption:\n",
    "Azure Storage encryption is enabled by default for all storage accounts using\n",
    "256-bit AES encryption. Data is encrypted using Microsoft-managed keys by default.\n",
    "\n",
    "Customer-managed keys with Azure Key Vault can be configured. These keys are\n",
    "stored in your Azure Key Vault instance. The storage service uses envelope\n",
    "encryption where data is encrypted with a DEK, which is then encrypted with a KEK.\n",
    "\n",
    "Access Management:\n",
    "Azure role-based access control (Azure RBAC) is the authorization system built on\n",
    "Azure Resource Manager that provides fine-grained access management. You can assign\n",
    "roles to users, groups, and applications at a certain scope.\n",
    "\n",
    "Shared Access Signatures (SAS) provide secure delegated access to resources in your\n",
    "storage account. With a SAS, you have granular control over how a client can access\n",
    "your data including validity interval and allowed IP addresses.\n",
    "            \"\"\",\n",
    "            metadata={'source': 'Azure Documentation', 'version': '2024-Q4'},\n",
    "            doc_type='technical'\n",
    "        ),\n",
    "\n",
    "        Document(\n",
    "            id=\"fintech_compliance\",\n",
    "            title=\"FinTech Compliance Requirements Document\",\n",
    "            content=\"\"\"\n",
    "Regulatory Compliance Framework for Digital Payment Platforms\n",
    "\n",
    "PCI DSS Requirements:\n",
    "All payment card data must be encrypted both in transit and at rest. The encryption\n",
    "standard must meet AES-256 minimum requirements. Network segmentation must isolate\n",
    "cardholder data environment from other networks.\n",
    "\n",
    "Requirement 3.4 specifically mandates that PAN must be rendered unreadable anywhere\n",
    "it is stored including portable digital media, backup media, and logs. Strong\n",
    "cryptography with associated key-management processes must be implemented.\n",
    "\n",
    "GDPR Compliance:\n",
    "The platform must implement privacy by design principles. Users must provide explicit\n",
    "consent for data processing. The right to erasure (right to be forgotten) must be\n",
    "technically implemented within 30 days of request.\n",
    "\n",
    "Data minimization principles require that only data necessary for the specified\n",
    "purpose should be collected. The platform must maintain detailed records of all\n",
    "data processing activities including purpose, categories, and retention periods.\n",
    "\n",
    "AML/KYC Procedures:\n",
    "Customer identification program (CIP) must verify identity within a reasonable time\n",
    "after account opening. This includes collecting name, date of birth, address, and\n",
    "identification number.\n",
    "\n",
    "Transaction monitoring systems must flag suspicious activities including structuring,\n",
    "rapid movement of funds, and transactions with high-risk jurisdictions. The system\n",
    "must generate SARs (Suspicious Activity Reports) within 30 days of detection.\n",
    "\n",
    "Data Retention:\n",
    "Transaction records must be retained for 5 years from the date of transaction. KYC\n",
    "documentation must be retained for 5 years after the business relationship ends.\n",
    "Audit logs must be retained for 7 years and must be immutable.\n",
    "            \"\"\",\n",
    "            metadata={'source': 'Compliance Department', 'version': '2024-Q3'},\n",
    "            doc_type='legal'\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QbVKdQELMDRw",
    "outputId": "9b78b9fc-90c2-4450-ff6b-b5396c5f30a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing 3 documents...\n",
      "Indexing Statistics:\n",
      "{\n",
      "  \"contextualized\": {\n",
      "    \"time_seconds\": 0.6755492687225342,\n",
      "    \"total_tokens\": 933,\n",
      "    \"embeddings_generated\": 49\n",
      "  },\n",
      "  \"standard\": {\n",
      "    \"time_seconds\": 0.6564440727233887,\n",
      "    \"embeddings_generated\": 49\n",
      "  }\n",
      "}\n",
      "\n",
      "Query: What encryption does AWS use for SSE-KMS?\n",
      "  Contextualized top score: 0.6144\n",
      "  Standard top score: 0.7523\n",
      "  Score improvement: -0.1380\n",
      "\n",
      "Query: How long must KYC documentation be retained?\n",
      "  Contextualized top score: 0.5676\n",
      "  Standard top score: 0.6816\n",
      "  Score improvement: -0.1140\n",
      "\n",
      "Query: What are the storage costs for Archive tier?\n",
      "  Contextualized top score: 0.5613\n",
      "  Standard top score: 0.5958\n",
      "  Score improvement: -0.0346\n",
      "\n",
      "Query: How quickly must right to erasure be implemented?\n",
      "  Contextualized top score: 0.5024\n",
      "  Standard top score: 0.7033\n",
      "  Score improvement: -0.2009\n",
      "\n",
      "Query: What is the data retion policy for audit logs?\n",
      "  Contextualized top score: 0.4802\n",
      "  Standard top score: 0.7008\n",
      "  Score improvement: -0.2206\n",
      "\n",
      "Query: What is the policty arond CIP?\n",
      "  Contextualized top score: 0.2228\n",
      "  Standard top score: 0.3853\n",
      "  Score improvement: -0.1625\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test queries that benefit from context\n",
    "def create_test_queries() -> List[str]:\n",
    "    \"\"\"\n",
    "    Queries designed to test contextualized vs standard retrieval\n",
    "    \"\"\"\n",
    "    return [\n",
    "        # Queries requiring document context\n",
    "        \"What encryption does AWS use for SSE-KMS?\",  # \"AES-256-GCM\" appears without \"AWS\" context\n",
    "        \"How long must KYC documentation be retained?\",  # \"5 years\" appears without \"KYC\" context\n",
    "        \"What are the storage costs for Archive tier?\",  # \"lowest storage costs\" needs Azure context\n",
    "        \"How quickly must right to erasure be implemented?\",  # \"30 days\" needs GDPR context\n",
    "        \"What is the data retion policy for audit logs?\", # \"7 years\"\n",
    "        \"What is the policty arond CIP?\",\n",
    "    ]\n",
    "\n",
    "# Main execution example\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize pipeline\n",
    "    pipeline = ContextualizedRAGPipeline(\n",
    "        api_key=userdata.get('VOYAGE'),\n",
    "        chunk_size=512,\n",
    "        chunk_overlap=0  # As recommended by Voyage\n",
    "    )\n",
    "\n",
    "    # Create and index test documents\n",
    "    documents = create_test_documents()\n",
    "    indexing_stats = pipeline.index_documents(documents)\n",
    "\n",
    "    print(\"Indexing Statistics:\")\n",
    "    print(json.dumps(indexing_stats, indent=2))\n",
    "\n",
    "    # Run comparison tests\n",
    "    queries = create_test_queries()\n",
    "    comparison = pipeline.compare_retrieval_methods(queries, top_k=5)\n",
    "\n",
    "    # print(\"\\n\" + \"=\"*50)\n",
    "    # print(\"RETRIEVAL COMPARISON RESULTS\")\n",
    "    # print(\"=\"*50)\n",
    "    # print(json.dumps(comparison['summary'], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6o1nQfGfMPQF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "025b5101",
    "outputId": "c5a5f70c-7377-4557-edc0-af0a930ecc3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What encryption does AWS use for SSE-KMS?\n",
      "  Contextualized:\n",
      "    Top Chunk: ['KMS uses \\nenvelope encryption with AES-256-GCM.', 'You can also use \\nserver-side encryption with AWS KMS keys (SSE-KMS) or customer-provided keys (SSE-C).', 'The data key is encrypted under a KMS key \\nthat never leaves AWS KMS unencrypted.']\n",
      "    Top Doc IDs: ['aws_s3_docs', 'aws_s3_docs', 'aws_s3_docs']\n",
      "    Top Score: 0.6143524661891981\n",
      "  Standard:\n",
      "    Top Chunk: ['KMS uses \\nenvelope encryption with AES-256-GCM.', 'For SSE-KMS, AWS KMS generates and manages the cryptographic keys.', 'The default encryption \\nuses server-side encryption with Amazon S3 managed keys (SSE-S3).']\n",
      "    Top Doc IDs: ['aws_s3_docs', 'aws_s3_docs', 'aws_s3_docs']\n",
      "    Top Score: 0.7523243177605102\n",
      "--------------------\n",
      "Query: How long must KYC documentation be retained?\n",
      "  Contextualized:\n",
      "    Top Chunk: ['KYC \\ndocumentation must be retained for 5 years after the business relationship ends.', 'Data Retention:\\nTransaction records must be retained for 5 years from the date of transaction.', 'Audit logs must be retained for 7 years and must be immutable.']\n",
      "    Top Doc IDs: ['fintech_compliance', 'fintech_compliance', 'fintech_compliance']\n",
      "    Top Score: 0.5675925777112331\n",
      "  Standard:\n",
      "    Top Chunk: ['KYC \\ndocumentation must be retained for 5 years after the business relationship ends.', 'Data Retention:\\nTransaction records must be retained for 5 years from the date of transaction.', 'Audit logs must be retained for 7 years and must be immutable.']\n",
      "    Top Doc IDs: ['fintech_compliance', 'fintech_compliance', 'fintech_compliance']\n",
      "    Top Score: 0.6816347369629747\n",
      "--------------------\n",
      "Query: What are the storage costs for Archive tier?\n",
      "  Contextualized:\n",
      "    Top Chunk: ['It offers the lowest storage costs.', 'Archive tier is optimized for data that is rarely accessed and stored for at least \\n180 days with flexible latency requirements.', 'It has lower storage costs but higher access costs than Hot.']\n",
      "    Top Doc IDs: ['azure_blob_docs', 'azure_blob_docs', 'azure_blob_docs']\n",
      "    Top Score: 0.5612787568758724\n",
      "  Standard:\n",
      "    Top Chunk: ['Archive tier is optimized for data that is rarely accessed and stored for at least \\n180 days with flexible latency requirements.', 'It offers the \\nhighest storage costs but lowest access costs.', 'Cool tier is optimized for storing data that is infrequently accessed and stored \\nfor at least 30 days.']\n",
      "    Top Doc IDs: ['azure_blob_docs', 'azure_blob_docs', 'azure_blob_docs']\n",
      "    Top Score: 0.5958323348434216\n",
      "--------------------\n",
      "Query: How quickly must right to erasure be implemented?\n",
      "  Contextualized:\n",
      "    Top Chunk: ['The right to erasure (right to be forgotten) must be \\ntechnically implemented within 30 days of request.', 'The platform must maintain detailed records of all \\ndata processing activities including purpose, categories, and retention periods.', 'GDPR Compliance:\\nThe platform must implement privacy by design principles.']\n",
      "    Top Doc IDs: ['fintech_compliance', 'fintech_compliance', 'fintech_compliance']\n",
      "    Top Score: 0.5024047229106231\n",
      "  Standard:\n",
      "    Top Chunk: ['The right to erasure (right to be forgotten) must be \\ntechnically implemented within 30 days of request.', 'The platform must maintain detailed records of all \\ndata processing activities including purpose, categories, and retention periods.', 'The system \\nmust generate SARs (Suspicious Activity Reports) within 30 days of detection.']\n",
      "    Top Doc IDs: ['fintech_compliance', 'fintech_compliance', 'fintech_compliance']\n",
      "    Top Score: 0.7032899797473652\n",
      "--------------------\n",
      "Query: What is the data retion policy for audit logs?\n",
      "  Contextualized:\n",
      "    Top Chunk: ['Audit logs must be retained for 7 years and must be immutable.', 'Data Retention:\\nTransaction records must be retained for 5 years from the date of transaction.', 'KYC \\ndocumentation must be retained for 5 years after the business relationship ends.']\n",
      "    Top Doc IDs: ['fintech_compliance', 'fintech_compliance', 'fintech_compliance']\n",
      "    Top Score: 0.48016365176943104\n",
      "  Standard:\n",
      "    Top Chunk: ['Audit logs must be retained for 7 years and must be immutable.', 'Data Retention:\\nTransaction records must be retained for 5 years from the date of transaction.', 'The platform must maintain detailed records of all \\ndata processing activities including purpose, categories, and retention periods.']\n",
      "    Top Doc IDs: ['fintech_compliance', 'fintech_compliance', 'fintech_compliance']\n",
      "    Top Score: 0.7007626311949635\n",
      "--------------------\n",
      "Query: What is the policty arond CIP?\n",
      "  Contextualized:\n",
      "    Top Chunk: ['AML/KYC Procedures:\\nCustomer identification program (CIP) must verify identity within a reasonable time \\nafter account opening.', 'This includes collecting name, date of birth, address, and \\nidentification number.', 'KYC \\ndocumentation must be retained for 5 years after the business relationship ends.']\n",
      "    Top Doc IDs: ['fintech_compliance', 'fintech_compliance', 'fintech_compliance']\n",
      "    Top Score: 0.2227820693469934\n",
      "  Standard:\n",
      "    Top Chunk: ['AML/KYC Procedures:\\nCustomer identification program (CIP) must verify identity within a reasonable time \\nafter account opening.', 'IAM policies specify what \\nactions are allowed or denied on AWS resources.', 'With a SAS, you have granular control over how a client can access \\nyour data including validity interval and allowed IP addresses.']\n",
      "    Top Doc IDs: ['fintech_compliance', 'aws_s3_docs', 'azure_blob_docs']\n",
      "    Top Score: 0.3853313288593661\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "for query_result in comparison['queries']:\n",
    "    print(f\"Query: {query_result['query']}\")\n",
    "    print(\"  Contextualized:\")\n",
    "    print(f\"    Top Chunk: {query_result['contextualized']['top_chunk']}\")\n",
    "    print(f\"    Top Doc IDs: {query_result['contextualized']['top_doc_ids']}\")\n",
    "    print(f\"    Top Score: {query_result['contextualized']['top_score']}\")\n",
    "    print(\"  Standard:\")\n",
    "    print(f\"    Top Chunk: {query_result['standard']['top_chunk']}\")\n",
    "    print(f\"    Top Doc IDs: {query_result['standard']['top_doc_ids']}\")\n",
    "    print(f\"    Top Score: {query_result['standard']['top_score']}\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I2HEO2PQQX48"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
